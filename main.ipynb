{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Corto 1\n",
    "\n",
    "Integrantes:\n",
    "- Francis Aguilar #22223\n",
    "- Gerardo Pineda #22808\n",
    "- Angela Garcia #22869"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo:\n",
    "## Implementar un agente inteligente que resuelva el juego de frozen lake con el argumento de slippery=True, usando un algoritmo de aprendizaje por refuerzo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias\n",
    "import gymnasium as gym #con esta liberia se tiene espacio de acciones, espacio observable, estado inicial conjunto de acciones, finales de episodio.\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seteo de semilla random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1741569842\n"
     ]
    }
   ],
   "source": [
    "# Configuración reproducible\n",
    "seed = int(time.time()) \n",
    "# Descomentar si no se quiere que sea aleatorio\n",
    "# seed = 42 \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(\"seed:\", seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapa: \n",
      "S F F F\n",
      "F F H F\n",
      "F F F F\n",
      "F H H G\n"
     ]
    }
   ],
   "source": [
    "#crear el entorno de frozen lake\n",
    "random_map = generate_random_map(size=4)\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\",  map_name=\"4x4\", is_slippery=True) # map_name=\"4x4\",\n",
    "\n",
    "print('mapa: ')\n",
    "for row in random_map:\n",
    "    print(\" \".join(row))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Q-learning:\n",
    "Este algoritmo es un método de aprendizaje por refuerzo que permite a un agente aprender a tomar decisiones óptimas en un entorno. \n",
    "Se usó este algoritmo porque este esta basado en los principio de MDP, procesos de decision de Markov, para aprender una politica óptima.\n",
    "\n",
    "\n",
    "El proceso de decisión de Markov (MDP), ayuda a modelar la toma de descisiones en situaciones donde los resultados son aleatorios y tiene bajo el control de decisor.Usualmente se usa en el area de IA, aprendizaje por refuerzo y la teoría de control.\n",
    "\n",
    "Componentes: \n",
    "- Estados (S): \n",
    "    Conjunto de todos los posibles estados en los que puede estar el agente.\n",
    "\n",
    "- Acciones (A):\n",
    "    las decisiones/ movimientos que un agente puede tomar\n",
    "\n",
    "- Probabilidades de transicion  (P):  \n",
    "    Describe la probabilidad de moverse de un estado a otra, dado una accion.\n",
    "\n",
    "- Recompensas (R):\n",
    "    Indican la recompensa inmediata luego de tomar una accion en un estado\n",
    "\n",
    "- Horizonte de tiempo: \n",
    "    Si es finito o infinito, define el periodo durante en el que se toman desiciones\n",
    "\n",
    "- Descuento:\n",
    "    factor de descuento entre 0 y 1 que reduce el valor de las recompensas futuras, lo que refleja es la preferencia por obtener las recomensas más temprano.\n",
    "\n",
    "\n",
    "Relación con Q-learning: \n",
    "Q-learning es un algoritmo de aprendizaje por refuerzo que se usa para encontrar la politica óptima (que es la que maximiza la recompenza acumulada a largo plazo) en un MDP. \n",
    "\n",
    "- Estados y Acciones: En Q-learning, el agente explora diferentes estados y acciones, que son componentes fundamentales de un MDP.\n",
    "\n",
    "- Transiciones: Q-learning no necesita conocer las probabilidades de transición tan a detalle, aprende por medio de la interaccion con el entorno, que sigue las \n",
    "propiedades de las cadenas de Markov.\n",
    "\n",
    "- Recompensas: El agente recibe recompensas basadas en las acciones tomadas y los estados alcanzados, y estas recompensas se utilizan para actualizar la tabla Q.\n",
    "\n",
    "- Factor de descuento: El factor de descuento (y) en Q-learning es el mismo concepto que en un MDP, que valora las recompensas futuras.\n",
    "\n",
    "\n",
    "Por lo tanto, se escogió este algoritmo por su parecido a MDP, por su entorno es estocástico, con el slippery = true. Su fácil implementacion, ser eficiente para problemas con estados y acciones, la convergencia de una política óptima, entonces el agente aprenderá de la mejor manera de actuar en el entorno. Al tener la estrategia de exploracion vs explotación, permite al agente balancear entre probar nuevas acciones y usar el conocimiento adquirido para maximizar la recompensa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 3, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 1, Accion: 2, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 2, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 1, Accion: 1, Reward: 0.0, Done: False\n",
      "Estado: 2, Accion: 3, Reward: 0.0, Done: False\n",
      "Estado: 6, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 2, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 2, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 6, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 10, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 6, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 10, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 6, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 10, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 9, Accion: 3, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 5, Accion: 2, Reward: 0.0, Done: True\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 5, Accion: 2, Reward: 0.0, Done: True\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 1, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 1, Accion: 3, Reward: 0.0, Done: False\n",
      "Estado: 1, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 1, Reward: 0.0, Done: False\n",
      "Estado: 0, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 4, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 1, Reward: 0.0, Done: False\n",
      "Estado: 8, Accion: 0, Reward: 0.0, Done: False\n",
      "Estado: 12, Accion: 0, Reward: 0.0, Done: True\n"
     ]
    }
   ],
   "source": [
    "# variables para el aprendizaje por refuerzo\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) # Q-valor para cada estado y acción\n",
    "alpha = 0.1 #tasa de aprendizaje\n",
    "gamma = 0.99 #factor de descuento\n",
    "epsilon = 0.1 #tasa de exploracion\n",
    "num_episodes = 10 #numero de episodios de entrenamiento\n",
    "\n",
    "\n",
    "\n",
    "#algorimo de Q-learning\n",
    "for episode in range(num_episodes):\n",
    "    #reinicia el entorno\n",
    "    state = env.reset()[0] \n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        #decidir s i toma una accion aleatoria o conocida\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            #random\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            #conocida\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "        #se ejecuta la accion y se obtiene la nueva posicion, recompensa, y si termino el episodio\n",
    "        new_state, reward, done, _, _ = env.step(action) #info y truncated no se usan, porque no se usan esos calores de gym, que eso estaria en vez de _\n",
    "        # se actualiza la tabla Q\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
    "        state = new_state\n",
    "        print(f\"Estado: {state}, Accion: {action}, Reward: {reward}, Done: {done}\")\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # print(\"Episodio: {}/{}\".format(episode, num_episodes))\n",
    "        # print(\"Q-Table\")\n",
    "        # print(Q)\n",
    "        # env.render()\n",
    "        # print(\"\\n\\n\\n\")\n",
    "        # env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluar el agente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
